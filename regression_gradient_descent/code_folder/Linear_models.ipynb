{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №2 - Линейные модели. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом домашнем задании мы с вами научимся обучать линейные модели регрессии и классификации при помощи очень мощного, но в то же время довольно понятного алгоритма, который называется **градиетный спуск**. Помимо линейных моделей он используется и для обучения самых сложных нейронных сетей! Также мы потренируемся применять готовые реализации линейных моделей для задач регрессии и бинарной классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.linear_model import (LinearRegression,\n",
    "                                  LogisticRegression)\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import (r2_score,\n",
    "                             mean_squared_error)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 12, 9\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "SEED = 111\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Маленькое теоретическое отступление\n",
    "\n",
    "Основное свойство антиградиента (-1 * градиент) &ndash; он указывает в сторону наискорейшего убывания функции в данной точке. Соответственно, будет логично стартовать из некоторой точки, сдвинуться в сторону антиградиента, пересчитать антиградиент и снова сдвинуться в его сторону и т.д. Запишем это более формально.\n",
    "\n",
    "Пусть $w_0$ &ndash; начальный набор параметров (коэффициентов линейной модели) ((например, нулевой или сгенерированный из некоторого, случайного распределения)). Тогда обычный градиентный спуск состоит в повторении следующих шагов до сходимости:\n",
    "\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta \\nabla_{w} Q(w_{k}),\n",
    "$$\n",
    "\n",
    "где $\\nabla_{w} Q(w_{k})$ &ndash; градиент функции потерь в точке $w_k$, а $\\eta$ &ndash; скорость обучения (learning rate).\n",
    "\n",
    "Градиентный спуск обычно останавливают, когда прошло заданное максимальное количество итераций или когда графиент близок к нулю (т.е. наши параметры практически не меняются). Для реализации второго варианта считают норму градиента (по сути длину вектора). Это можно сделать несколькими способами:\n",
    "\n",
    "$$\n",
    "l1_{norm} = \\sum{|w_i|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "l2_{norm} = \\sum{(w_i)^{2}}\n",
    "$$\n",
    "\n",
    "Попробуем разобраться на простом примере. Рассмотрим функцию от двух переменных:\n",
    "$f(x, y) = \\sin^2 x + \\sin^2 y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w):\n",
    "    \"\"\"\n",
    "    :param w: np.array(np.float) вектор из 2-х элементов\n",
    "    :return: np.float\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sum(np.sin(w)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что $x$ - numpy-array вектор длины 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Reminder:***  \n",
    "Что мы хотим? Мы хотим найти минимум этой функции (в машинном обучении мы обычно хотим найти минимум **функции потерь**, например, MSE), а точнее найти $w_1$ и $w_2$ такие, что при них значение $f(w_1, w_2)$ минимально, то есть *точку экстремума*.  \n",
    "  \n",
    "Как мы будем искать эту точку? Используем методы оптимизации (в нашем случае - *минимизации*). Одним из таких методов и является **градиентный спуск**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Градиентный спуск для функции $f$ (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию, которая будет осуществлять градиентный спуск для функции $f$:\n",
    "\n",
    "*Примечание:* Вам нужно посчитать частные производные именно **аналитически** и **переписать их в код**, а не считать производные численно (через отношение приращения функции к приращению аргумента) -- в этих двух случаях могут различаться ответы, поэтому будьте внимательны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f(w): \n",
    "    \"\"\"\n",
    "    Градиент функциии f, определенной выше.\n",
    "        :param w: np.array[2]: float вектор из 2-х элементов\n",
    "        :return: np.array[2]: float вектор из 2-х элементов\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    YOUR CODE IS HERE\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что градиент принимает вектор из двух чисел и выдает на этой точке верное значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(grad_f(np.array([1, 2])), \n",
    "                   np.array([0.90929743, -0.7568025])), \"Что-то не так!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent_2d(f, grad_f, lr, num_iter=100, x0=None):\n",
    "    \"\"\"\n",
    "    Функция, которая реализует градиентный спуск в минимум для функции f от двух переменных. \n",
    "        :param f: скалярная функция двух переменных\n",
    "        :param grad_f: функция, возвращающая градиент функции f (устроена как реализованная вами выше grad_f)\n",
    "        :param lr: learning rate алгоритма\n",
    "        :param num_iter: количество итераций градиентного спуска\n",
    "        :return: np.array[num_iter, 2] пар вида (x, f(x))\n",
    "    \"\"\"\n",
    "    \n",
    "    w0 = np.random.random(2)\n",
    "\n",
    "    # будем сохранять значения аргументов и значений функции \n",
    "    # в процессе град. спуска в переменную history\n",
    "    history = []\n",
    "\n",
    "    # итерация цикла == шаг градиентнго спуска\n",
    "    curr_w = w0.copy()\n",
    "    for iter_num in range(num_iter):\n",
    "        entry = np.hstack((curr_w, f(curr_w)))\n",
    "        history.append(entry)\n",
    "    \n",
    "        curr_w -= #  YOUR CODE. Не забудьте про lr!\n",
    "\n",
    "    return np.vstack(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем точки градиентного спуска на 3D-графике нашей функции. Звездочками будут обозначены точки (тройки $w_1, w_2, f(w_1, w_2)$), по которым Ваш алгоритм градиентного спуска двигался к минимуму (Для того, чтобы написовать этот график, мы и сохраняли значения $cur\\_w_1, cur\\_w_2, f(cur\\_w_1, cur\\_w_2)$ в `steps` в процессе спуска).\n",
    "\n",
    "Если у Вас правильно написана функция `grad_descent_2d`, то звездочки на картинке должны сходиться к одной из точек минимума функции. Вы можете менять начальные приближения алгоритма, значения `lr` и `num_iter` и получать разные результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_desc_vis(f, grad_f, lr=0.1, num_iter=20):\n",
    "    steps = grad_descent_2d(f, grad_f, lr=lr, num_iter=num_iter)\n",
    "    \n",
    "    X, Y = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    ax = fig.gca(projection=\"3d\")\n",
    "\n",
    "    zs = np.array([f(np.array([x,y]))\n",
    "                  for x, y in zip(np.ravel(X), np.ravel(Y))])\n",
    "    Z = zs.reshape(X.shape)\n",
    "\n",
    "\n",
    "    ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, zorder=2)\n",
    "\n",
    "    ax.plot(xs=steps[:, 0], ys=steps[:, 1], zs=steps[:, 2],\n",
    "            marker=\"*\", markersize=20, zorder=3, \n",
    "            markerfacecolor=\"y\", lw=3, c=\"black\")\n",
    "\n",
    "    ax.set_zlim(0, 5)\n",
    "    ax.view_init(elev=60)\n",
    "    plt.show()\n",
    "    \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = gradient_desc_vis(f, grad_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на зависимость значения функции от шага градиентного спуска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.xlabel(\"grad descent step number\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "plt.title(\"Значение функции на каждом шаге гардиентного спуска.\")\n",
    "\n",
    "f_values = list(map(lambda x: x[2], steps))\n",
    "plt.plot(f_values, label=\"gradient descent result\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Реализация линейной регресии (суммарно 9 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как мы будем использовать градиентный спуск для обучения модели, важной часть является реализация функции потерь и функции для расчета ее градиента. Перем началом стоит напомнить, как считать градиент MSE. Вывод этой формулы можно найти  [здесь](https://medium.com/analytics-vidhya/linear-regression-gradient-descent-intuition-and-math-c9a8f5aeeb22)\n",
    "\n",
    "$$\n",
    "    MSE = \\frac{1}{N}\\sum(y_{true} - y_{pred}) ^ 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\nabla{MSE} = \\frac{2}{N} X^T (y_{pred} - y_{true})\n",
    "$$\n",
    "\n",
    "Здесь имеется в виду именно матричное умножение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2.1. MSE и ее градиент (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Функция потерь MSE.\n",
    "        :param y_true: np.array[n_samples]: вектор из правильных ответов\n",
    "        :param y_pred: np.array[n_samples]: вектор из предсказаний модели\n",
    "        :return: значение функции потерь\n",
    "    \"\"\"\n",
    "    \n",
    "    if  y_true.shape[0] != y_pred.shape[0]:\n",
    "        raise ValueError(\"Number of samples in both vectors should be equal\")\n",
    "        \n",
    "    \"\"\"\n",
    "    YOUR CODE IS HERE\n",
    "    \"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def mse_grad(y_true, y_pred, X):\n",
    "    \"\"\"\n",
    "    Функция для расчета градиента MSE.\n",
    "        :param y_true: np.array[n_samples]: вектор из правильных ответов\n",
    "        :param y_pred: np.array[n_samples]: вектор из предсказаний модели\n",
    "        :param X: np.array[n_samples, n_features]: матрица объекты x признаки\n",
    "        :return: градиент функции потерь MSE\n",
    "    \"\"\"\n",
    "    \n",
    "    if  y_true.shape[0] != y_pred.shape[0]:\n",
    "        raise ValueError(\"Number of samples in both vectors should be equal\")\n",
    "        \n",
    "    \"\"\"\n",
    "    YOUR CODE IS HERE\n",
    "    \"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "class MSELoss:\n",
    "    \"\"\"\n",
    "    Класс, реализующий функцию потерь MSE. Нужен для того, чтобы\n",
    "    объединять в одном месте функцию потерь и градиент для нее.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return mse(y_true, y_pred)\n",
    "    \n",
    "    def calculate_gradient(self, y_true, y_pred, X):\n",
    "        return mse_grad(y_true, y_pred, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать следующий класс для расчета градиента наших функций потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGradientDescent:\n",
    "    \"\"\"\n",
    "    Класс, позволяющий делать шаги градиентного спуска,\n",
    "    а также рассчитывающих норму градиента.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss_function, grad_norm):\n",
    "        self.loss = loss_function\n",
    "        self.grad_norm = grad_norm\n",
    "        \n",
    "    \n",
    "    def step(self, y, y_pred, X):\n",
    "        grad_i = self.loss.calculate_gradient(y, y_pred, X)\n",
    "        grad_i_norm = self._calculate_grad_norm(grad_i)\n",
    "        \n",
    "        return grad_i, grad_i_norm\n",
    "            \n",
    "            \n",
    "    def _calculate_grad_norm(self, grad_i):\n",
    "        if self.grad_norm == \"l1\":\n",
    "            return np.abs(grad_i).sum()\n",
    "        elif self.grad_norm == \"l2\":\n",
    "            return np.sqrt(np.square(grad_i).sum())\n",
    "        else:\n",
    "            raise ValueError(f\"I can't calculate {self.grad_norm} norm of gradient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании нужно будет реализовать линейную регрессию и обучить ее при помощи градиентного спуска. Для этого нужно будет заполнять пропуски кода в соответствующих классах. Для начала мы реализуем базовый класс для всех линейных моделей, от которого потом будем наследоваться при реализации линейной и логистической регресий. Не переживайте, этот класс уже реализован, вам достостаточно просто разобраться с кодом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLinearModel:\n",
    "    \"\"\"\n",
    "    Класс, который представляет из себя базовую линейную модель, наследуюясь от которого, мы будем\n",
    "    реализовывать линейную и логистическую регрессии.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate, \n",
    "                 loss_function, fit_intercept,\n",
    "                 n_iter, tol, optimizer, grad_norm):\n",
    "        \"\"\"\n",
    "        Конструктор нашего класса. \n",
    "            :param learning_rate: скорость обучения\n",
    "            :param loss_function: функция потерь (MSE или кросс-энтропия)\n",
    "            :param fit_intercept: нужно ли нам включать свободных член в модель\n",
    "            :param n_iter: количество итераций градиентного спуска\n",
    "            :param tol: параметр для остановки градиентного спуска,\n",
    "                        если норма градиента (l1 или l2) меньше tol, то останавливаемся\n",
    "            :param optimizer: класс, который будет рассчитывать градиент и его норму\n",
    "            :param grad_norm: тип нормы градиента l1 или l2\n",
    "        \"\"\"\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss_function\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "        self.grad_norm = grad_norm\n",
    "        self.optimizer = optimizer(loss_function, grad_norm)\n",
    "        \n",
    "        # В начале параметры модели не заданы\n",
    "        self.W = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Метод для обучения нашей модели \n",
    "            :param X: матрица объекты x признаки\n",
    "            :param y: вектор значений целевой переменной\n",
    "            :return: обученная модель\n",
    "        \"\"\"\n",
    "        \n",
    "        # Сделаем из y вектор-столбец (n_samples, 1)\n",
    "        y = y.reshape(-1, 1)\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Добавим колонку из 1 в матрицу X\n",
    "        if self.fit_intercept:\n",
    "            ones_column = np.ones((n_samples, 1))\n",
    "            X_new = np.hstack((ones_column, X))\n",
    "        \n",
    "        n_features = X_new.shape[1]\n",
    "        \n",
    "        # Инициализируем веса модели\n",
    "        if self.W is None:\n",
    "            self.W = np.random.randn(n_features, 1)\n",
    "        \n",
    "        # Обучаем модель градиентным спуском\n",
    "        for i in range(self.n_iter):\n",
    "            y_pred = self.predict(X)\n",
    "            grad_i, grad_i_norm = self.optimizer.step(y, y_pred, X_new)\n",
    "            \n",
    "            # Если градиент близок к 0, останавливаемся\n",
    "            if grad_i_norm <= self.tol:\n",
    "                return self\n",
    "            \n",
    "            else:\n",
    "                self.W -= self.learning_rate * grad_i\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError(\"It is a basic class for all linear models. You should implement it for descendant class.\")\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Base linear model without prediction skill :(\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2.2. Предсказания линейной регрессии (3 балла)\n",
    "\n",
    "Реализуйте метод `predict` у класса `CustomLinearRegression`, не забудьте про свободный член!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinearRegression(BaseLinearModel):\n",
    "    def __init__(self, learning_rate: float = 1e-2, \n",
    "                 loss_function=MSELoss(), fit_intercept=True,\n",
    "                 n_iter=1000, tol=1e-5, optimizer=BasicGradientDescent, grad_norm=\"l1\"):\n",
    "        \n",
    "        # Если вы не проходили наследование и в частности `super`, то не страшно\n",
    "        # коротко, с помощью этого мы можем вызывать методы родительского класса\n",
    "        # в частности здесь мы используем метод `init`\n",
    "        super().__init__(learning_rate=learning_rate, \n",
    "                         loss_function=loss_function, fit_intercept=fit_intercept,\n",
    "                         n_iter=n_iter, tol=tol, optimizer=optimizer, grad_norm=grad_norm)\n",
    "        \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Метод для вычисления предсказаний \n",
    "            :param X_test: np.array[n_test_samples, n_features]: \n",
    "                           матрица объекты x признаки (тестовый датасет)\n",
    "            :return: y_pred: np.array[n_test_samples, 1]: предсказания модели\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.W is None:\n",
    "            raise NotFittedError(\"This CustomLinearRegression instance is not fitted yet, run fit method.\")\n",
    "        \n",
    "        n_test_samples = X_test.shape[0]\n",
    "        if self.fit_intercept:\n",
    "            ones_column = np.ones((n_test_samples, 1))\n",
    "            X_test = np.hstack((ones_column, X_test))\n",
    "            \n",
    "        \"\"\"\n",
    "        YOUR CODE IS HERE\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"My custom linear regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Проверим нашу реализацию на простом примере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(100, 1)\n",
    "y = 2 * X + 5 + 0.5 * np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_lin_reg = CustomLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_lin_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X, custom_lin_reg.predict(X));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2.3. Используем встроенную линейную регрессию (4 балла)\n",
    "\n",
    "Поработаем с данными о ценах на дома в Бостоне. Постройте модель линейной регресии при помощи `LinearRegression` из `sklearn`. Не забудьте разделить данные на тренировочную и тестовую части, а также правильно предобработать признаки. В конце воспользуйтесь какими-то изученными метриками регресии и сделайте выводы о качестве полученной модели, а также о том, какие признаки наиболее важны с точки зрения полученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "X, y = data[\"data\"], data[\"target\"]\n",
    "feature_names = data[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваш ход:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3. Реализация логистической регресии (суммарно 10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия не очень сильно отличается от обычной линейной регрессии и используется в задах классификации. Так как здесь мы снова будем пользоваться градиентным спуском, то нужно определить функцию потерь и ее градиент. Одним из самых популярных вариантов в задаче бинарной классификации является бинарная кросс-энтропия (BCE).\n",
    "\n",
    "$$\\mathcal L_{BCE}(y, \\hat y) = -\\sum_i \\left[y_i\\log\\sigma(\\hat y_i) + (1-y_i)\\log(1-\\sigma(\\hat y_i))\\right].$$\n",
    "\n",
    "где $y$ это  таргет желаемого результата и $\\hat y$ является выходом модели. $\\sigma$ - это [*логистическая* функция](https://en.wikipedia.org/wiki/Sigmoid_function), который преобразует действительное число $\\mathbb R$ в вероятность $[0,1]$.\n",
    "\n",
    "Единственная проблема данной функции это возможность получить 0 под знаком логарифма, что не очень хорошо. Попробуем справить с этим \"в лоб\". Скажем, что наши предсказания могут принимать значения от 0 + eps до 1 - eps, где eps очень маленькое число."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.1. Реализация сигмоиды (0.5 баллов)\n",
    "\n",
    "Реализуйте функцию `sigmoid`, которая переводит действительное число $\\mathbb R$ в вероятность $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(output):\n",
    "    # output результат X@w (-inf, +inf)\n",
    "    \"\"\"\n",
    "    YOUR CODE IS HERE\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.2. BCE Loss и ее градиент (2.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как мы с вами только начинаем изучать машинное обучение, то было бы слишком жестоко просить вас вычислить градиент BCE Loss (он не так сложен, просто нужно привыкнуть). Поэтому сразу напишем формулу для него:\n",
    "\n",
    "$$\n",
    "\\nabla{\\mathcal L_{BCE}(y, \\hat y), X} = X^T (\\sigma({\\hat{y}}) - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Функция потерь BCE.\n",
    "        :param y_true: np.array[n_samples]: вектор из правильных ответов 0/1\n",
    "        :param y_pred: np.array[n_samples]: вектор из предсказаний модели (вероятности)\n",
    "        :return: значение функции потерь\n",
    "    \"\"\"\n",
    "    \n",
    "    if  y_true.shape[0] != y_pred.shape[0]:\n",
    "        raise ValueError(\"Number of samples in both vectors should be equal\")\n",
    "        \n",
    "    n = y_true.shape[0]\n",
    "    \n",
    "    # So I want escape log(0)\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    \n",
    "    \"\"\"\n",
    "    YOUR CODE IS HERE\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "\n",
    "def bce_grad(y_true, y_pred, X):\n",
    "    \"\"\"\n",
    "    Функция потерь BCE.\n",
    "        :param y_true: np.array[n_samples]: вектор из правильных ответов 0/1\n",
    "        :param y_pred: np.array[n_samples]: вектор из предсказаний модели (вероятности)\n",
    "        :param X: np.array[n_samples, n_features]: матрица объекты x признаки\n",
    "        :return: значение функции потерь\n",
    "    \"\"\"\n",
    "    \n",
    "    if  y_true.shape[0] != y_pred.shape[0]:\n",
    "        raise ValueError(\"Number of samples in both vectors should be equal\")\n",
    "        \n",
    "    \"\"\"\n",
    "    YOUR CODE IS HERE\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BCELoss:\n",
    "    \"\"\"\n",
    "    Класс, реализующий функцию потерь BCE. Нужен для того, чтобы\n",
    "    объединять в одном месте функцию потерь и градиент для нее.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return bce(y_true, y_pred)\n",
    "    \n",
    "    def calculate_gradient(self, y_true, y_pred, X):\n",
    "        return bce_grad(y_true, y_pred, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.3. Предсказания логистической регрессии (2 балла)\n",
    "\n",
    "Реализуйте метод `predict` у класса `CustomLogisticRegression`, не забудьте про свободный член!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression(BaseLinearModel):\n",
    "    def __init__(self, learning_rate: float = 1e-3,\n",
    "                 loss_function=BCELoss(), fit_intercept=True,\n",
    "                 n_iter=1000, tol=1e-5, optimizer=BasicGradientDescent, grad_norm=\"l1\"):\n",
    "        \n",
    "        super().__init__(learning_rate=learning_rate,\n",
    "                         loss_function=loss_function, fit_intercept=fit_intercept,\n",
    "                         n_iter=n_iter, tol=tol, optimizer=optimizer, grad_norm=grad_norm)\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        if self.W is None:\n",
    "            raise NotFittedError(\"This CustomLogisticRegression instance is not fitted, run fit method.\")\n",
    "        \n",
    "        n_test_samples = X_test.shape[0]\n",
    "        if self.fit_intercept:\n",
    "            ones_column = np.ones((n_test_samples, 1))\n",
    "            X_test = np.hstack((ones_column, X_test))\n",
    "            \n",
    "        \"\"\"\n",
    "        YOUR CODE IS HERE\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"My custom logistic regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Снова проверим работу алгоритма на простом примере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим датасет из 1 признака и 2 классов\n",
    "X, y = make_classification(n_features=1, n_informative=1,\n",
    "                           n_redundant=0, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_log_reg = CustomLogisticRegression()\n",
    "custom_log_reg.fit(X, y)\n",
    "y_pred = custom_log_reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.scatter(X, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте качество работы модели при помощи известных вам метрик бинарной классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.4. Применение логистической регрессии (5 баллов)\n",
    "\n",
    "Мы будем использовать данные по свойствам покемонов (https://www.kaggle.com/abcsds/pokemon). В данном задании вам необходимо сначала сделать краткий EDA (Посмотреть на данные и их распределения, а также посмотреть, как различные признаки связаны между собой и с целевой переменной (`Legendary`))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon = pd.read_csv(\"Pokemon.csv\")\n",
    "pokemon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем предсказывать является ли покемон легендарным или нет. Замените логическое значение колонки на числовое (перекодировав на 0 и 1). Также подумайте, как в этом случае лучше закодировать категориальные признаки (может быть, лучше их просто выбросить?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите ваши данные на тестовую и тренировочную выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите модель `LogisticRegression` из `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите метрики вашего классификатора:\n",
    "\n",
    "1. Нарисуйте [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html).\n",
    "\n",
    "2. Изобразите ROC кривую и посчитайте площадь под ней.\n",
    "\n",
    "3. Скажите, какие признаки оказались наиболее важны для модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4. Расскажите о вашей любимой музыкальной группе (исполнителе) (0.5 балла)\n",
    "\n",
    "Расскажите, как вы познакомились с этой группой и скиньте несколько наиболее любимых треков)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Therapy time\n",
    "\n",
    "Напишите здесь ваши впечатления о задании: было ли интересно, было ли слишком легко или наоборот сложно и тд. Также сюда можно написать свои идеи по улучшению заданий, а также предложить данные, на основе которых вы бы хотели построить следующие дз. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ваши мысли:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
